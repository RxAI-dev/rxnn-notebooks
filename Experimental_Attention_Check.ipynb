{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "Kr6rVV8vKrzZ",
        "EEZvQQAGodyY"
      ],
      "machine_shape": "hm",
      "gpuType": "L4",
      "authorship_tag": "ABX9TyMnDrV2vqI6H9fmmn6Nr6B0",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/RxAI-dev/rxnn-notebooks/blob/main/Experimental_Attention_Check.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Install RxNN and dependencies"
      ],
      "metadata": {
        "id": "1Z307QCJRDZ7"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "t2dIyd3HOyod"
      },
      "outputs": [],
      "source": [
        "!pip install rxnn==0.1.59 torch==2.6.0 transformers tokenizers huggingface_hub datasets"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!wget https://github.com/Dao-AILab/flash-attention/releases/download/v2.7.4.post1/flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "cSI_289_GPX_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install --no-dependencies --upgrade flash_attn-2.7.4.post1+cu12torch2.6cxx11abiFALSE-cp311-cp311-linux_x86_64.whl"
      ],
      "metadata": {
        "id": "uajKSTd1Goh7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Import libraries"
      ],
      "metadata": {
        "id": "CS1gqGGlRIhX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import torch, numpy as np, random, gc, os\n",
        "from rxnn.experimental.models import ExperimentalAttentionTransformer\n",
        "from rxnn.training.dataset import AutoregressiveLMDataset\n",
        "from rxnn.training.bml import AutoregressiveTrainer\n",
        "from rxnn.training.scheduler import get_transformer_lr_scheduler\n",
        "from rxnn.training.callbacks import PrintLossCallback, PrintAccuracyCallback, TokenCounterCallback, ModelSaveCallback\n",
        "from rxnn.training.tokenizer import TokenizerTrainer, load_tokenizer_from_hf_hub\n",
        "from rxnn.utils import get_model_size\n",
        "from datetime import datetime"
      ],
      "metadata": {
        "id": "tbZe8SHzi5ND"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "torch.__version__"
      ],
      "metadata": {
        "id": "kIEhoyOngqXs"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import flash_attn\n",
        "flash_attn.__version__"
      ],
      "metadata": {
        "id": "6BjDyknb7E1V"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixture-of-Experts Attention - Test on micro-size models (~2.5M Params) with tinyStories dataset\n",
        "\n",
        "> Unfortunately Mixture-of-Experts Attention has worse results than classic GQA/MQA, so we abandoned this study and proceed to **Sparse Query Attention** research.\n",
        "\n"
      ],
      "metadata": {
        "id": "6ohj1MQZJd2U"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "vocab_size = 5_000\n",
        "seq_len = 256\n",
        "\n",
        "torch.random.manual_seed(2137)\n",
        "np.random.seed(2137)\n",
        "random.seed(2137)\n",
        "\n",
        "base_config = {\n",
        "    'num_layers': 6,\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': embed_dim,\n",
        "    'att_heads': 8,\n",
        "    'att_groups': 2,\n",
        "    'seq_len': seq_len,\n",
        "    'use_flash_attention': False,\n",
        "    'use_gated': True,\n",
        "    'ff_dropout': 0.1,\n",
        "    'ff_activation': 'silu',\n",
        "    'ff_dim': 384,\n",
        "}\n",
        "\n",
        "gqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'gqa',\n",
        "}\n",
        "\n",
        "mqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'mqa',\n",
        "}\n",
        "\n",
        "mha_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'mha',\n",
        "}\n",
        "\n",
        "\n",
        "gma_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'gma',\n",
        "}\n",
        "\n",
        "dma_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'dma',\n",
        "    'att_num_query_groups': 4\n",
        "}\n",
        "\n",
        "\n",
        "gqa_decoder = ExperimentalAttentionTransformer(**gqa_config)\n",
        "mqa_decoder = ExperimentalAttentionTransformer(**mqa_config)\n",
        "mha_decoder = ExperimentalAttentionTransformer(**mha_config)\n",
        "gma_decoder = ExperimentalAttentionTransformer(**gma_config)\n",
        "dma_decoder = ExperimentalAttentionTransformer(**dma_config)\n",
        "\n",
        "(('GQA', gqa_decoder.params_count()),\n",
        "('MQA', mqa_decoder.params_count()),\n",
        "('MHA', mha_decoder.params_count()),\n",
        "('GMA', gma_decoder.params_count()),\n",
        "('DMA', dma_decoder.params_count()))"
      ],
      "metadata": {
        "id": "S_BQAoKQh4hu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Computational efficiency"
      ],
      "metadata": {
        "id": "qBBjYqnw9TTf"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "torch.cuda.empty_cache()\n",
        "print(device)\n",
        "\n",
        "inp = torch.ones(256, 256, dtype=torch.int32).to(device)\n",
        "gqa_decoder = gqa_decoder.to(device)\n",
        "mha_decoder = mha_decoder.to(device)\n",
        "mqa_decoder = mqa_decoder.to(device)\n",
        "dma_decoder = dma_decoder.to(device)\n",
        "gma_decoder = gma_decoder.to(device)\n",
        "\n",
        "steps = 100\n",
        "warmup = 100\n",
        "with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "  print(inp.dtype)\n",
        "  for _ in range(warmup):\n",
        "    _ = dma_decoder(inp)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = dma_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  print('DMA: ', t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = gma_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  print('GMA: ', t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = gqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  print('GQA: ', t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "      _ = mha_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  print('MHA: ', t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = mqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  print('MQA: ', t2 - t1, (t2 - t1) / steps)"
      ],
      "metadata": {
        "id": "AFFGFjfvtHu2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get tokenizer"
      ],
      "metadata": {
        "id": "nJPeUqEF9ZJA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tr = TokenizerTrainer.from_pretrained('ReactiveAI/RxT-Alpha-Micro-Decoder')\n",
        "tokenizer = tr.get_hf_tokenizer()\n",
        "tokenizer\n"
      ],
      "metadata": {
        "id": "i0qGEORtkj__"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load datasets"
      ],
      "metadata": {
        "id": "o7NZTl6I9cdx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "stories_dataset = load_dataset('roneneldan/TinyStories', split='train', trust_remote_code=True)\n",
        "hf_valid_dataset = load_dataset('roneneldan/TinyStories', split='validation', trust_remote_code=True)\n",
        "len(stories_dataset), len(hf_valid_dataset)"
      ],
      "metadata": {
        "id": "84ra1l3Pkwm5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MultiHeadAttention model"
      ],
      "metadata": {
        "id": "eG49INM49eyK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: MHA\n",
        "decoder = mha_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att/tensorboard_logs/mha'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att/mha', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/MHA-MAT', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='MHA Ref Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "0aX8T2yflIch"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GroupedQueryAttention model"
      ],
      "metadata": {
        "id": "8RT-tPwhJMGR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GQA\n",
        "decoder = gqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att/tensorboard_logs/gqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att/gqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GQA-MAT', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GQA Ref Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "-_npTT3wJQQe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MultiQueryAttention model"
      ],
      "metadata": {
        "id": "_BqGzwARJaRL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: MQA\n",
        "decoder = mqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att/tensorboard_logs/mqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att/mqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/MQA-MAT', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='MQA Ref Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "n4F_z1PVJdKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GroupedMoeAttention model"
      ],
      "metadata": {
        "id": "YY0w91wMJlaw"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GMA\n",
        "decoder = gma_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att/tensorboard_logs/gma'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att/gma', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GMAT', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GroupedMoeAttentionTransformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "uuOphC4rJqlq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train DeepMoeAttention model"
      ],
      "metadata": {
        "id": "KGW2u-zyKNBW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: DMA\n",
        "decoder = dma_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att/tensorboard_logs/dma'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att/dma', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/DMAT', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='DeepMoeAttentionTransformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "EGss2tl-KSje"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train Extended DMA model"
      ],
      "metadata": {
        "id": "TfckgdTsQwzX"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: Extended DMA\n",
        "ext_config = {\n",
        "    'num_layers': 6,\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': embed_dim,\n",
        "    'att_heads': 8,\n",
        "    'att_groups': 8,\n",
        "    'seq_len': seq_len,\n",
        "    'use_flash_attention': False,\n",
        "    'use_gated': True,\n",
        "    'ff_dropout': 0.1,\n",
        "    'ff_activation': 'silu',\n",
        "    'ff_dim': 384,\n",
        "    'att_type': 'dma',\n",
        "    'att_num_experts': 16,\n",
        "    'att_num_query_experts': 16,\n",
        "    'att_num_query_groups': 8\n",
        "}\n",
        "decoder = ExperimentalAttentionTransformer(**ext_config)\n",
        "print(decoder.params_count())\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att/tensorboard_logs/xdma'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att/xdma', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/xDMAT', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='ExtendedDeepMoeAttentionTransformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "6i51pq4xQ0cV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixture-of-Experts Attention - Test on mini-size models (~11-12M Params) on wikipedia dataset\n",
        "\n",
        "> For faster results, use only 50% of wikipedia dataset (>3M items) - 45% for training and 5% for validation.\n",
        "\n",
        "> Unfortunately Mixture-of-Experts Attention has worse results than classic GQA/MQA, so we abandoned this study and proceed to **Sparse Query Attention** research.\n",
        "\n",
        "> **MHA/GQA/MQA** results from this run were later used to compare with **SQA** variants"
      ],
      "metadata": {
        "id": "FeqM-krgJ2ua"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 256\n",
        "vocab_size = 10_000\n",
        "seq_len = 1024\n",
        "\n",
        "torch.random.manual_seed(210037)\n",
        "np.random.seed(210037)\n",
        "random.seed(210037)\n",
        "\n",
        "base_config = {\n",
        "    'num_layers': 8,\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': embed_dim,\n",
        "    'att_heads': 16,\n",
        "    'att_groups': 4,\n",
        "    'seq_len': seq_len,\n",
        "    'use_flash_attention': False,\n",
        "    'use_gated': True,\n",
        "    'ff_dropout': 0.1,\n",
        "    'ff_activation': 'silu',\n",
        "    'ff_dim': 768,\n",
        "}\n",
        "\n",
        "gqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'gqa',\n",
        "}\n",
        "\n",
        "mqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'mqa',\n",
        "}\n",
        "\n",
        "mha_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'mha',\n",
        "}\n",
        "\n",
        "\n",
        "gma_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'gma',\n",
        "}\n",
        "\n",
        "dma_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'dma',\n",
        "    'att_num_query_groups': 8\n",
        "}\n",
        "\n",
        "\n",
        "gqa_decoder = ExperimentalAttentionTransformer(**gqa_config)\n",
        "mqa_decoder = ExperimentalAttentionTransformer(**mqa_config)\n",
        "mha_decoder = ExperimentalAttentionTransformer(**mha_config)\n",
        "gma_decoder = ExperimentalAttentionTransformer(**gma_config)\n",
        "dma_decoder = ExperimentalAttentionTransformer(**dma_config)\n",
        "\n",
        "(('GQA', gqa_decoder.params_count()),\n",
        "('MQA', mqa_decoder.params_count()),\n",
        "('MHA', mha_decoder.params_count()),\n",
        "('GMA', gma_decoder.params_count()),\n",
        "('DMA', dma_decoder.params_count()))"
      ],
      "metadata": {
        "id": "KFMBKMp4J8b7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Use pretrained tokenizer from RxT-Alpha Mini"
      ],
      "metadata": {
        "id": "bVjsKF2MKgmi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tokenizer = load_tokenizer_from_hf_hub('ReactiveAI/RxT-Alpha-Mini-Decoder')\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "ZTJXZL0RKeM0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Load and split dataset"
      ],
      "metadata": {
        "id": "Kr6rVV8vKrzZ"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "wiki_dataset = load_dataset('wikimedia/wikipedia', '20231101.en', split='train', trust_remote_code=True)\n",
        "train_split_len = int(len(wiki_dataset) * 0.45)\n",
        "valid_split_len = int(len(wiki_dataset) * 0.05)\n",
        "wiki_train_dataset = wiki_dataset.select(range(train_split_len))\n",
        "wiki_valid_dataset = wiki_dataset.select(range(train_split_len, train_split_len + valid_split_len))\n",
        "_rest_dataset = wiki_dataset.select(range(train_split_len + valid_split_len, len(wiki_dataset)))\n",
        "len(wiki_train_dataset), len(wiki_valid_dataset), len(_rest_dataset)"
      ],
      "metadata": {
        "id": "cpBu8eobKocu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GQA Mini model\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "818YD1FTMlL4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GQA\n",
        "decoder = gqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/gqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/gqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GQA-MAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GQA Mini Ref Transformer v')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "PQivlqolMkKl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MHA Mini model"
      ],
      "metadata": {
        "id": "Rzy5Yim4PgeG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: MHA\n",
        "decoder = mha_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/mha'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/mha', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/MHA-MAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='MHA Mini Ref Transformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "T4bIH2iJO7fa"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train MQA Mini model"
      ],
      "metadata": {
        "id": "KnGOy4ppPq9O"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: MQA\n",
        "decoder = mqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/mqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/mqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/MQA-MAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='MQA Mini Ref Transformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "OArq3JYbPx82"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train GMA Mini model"
      ],
      "metadata": {
        "id": "jdmIeUa4QEzs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GMA\n",
        "decoder = gma_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/gma'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/gma', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GMAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GroupedMoeAttentionTransformer Mini v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "gBAARTSmQPO3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train DMA Mini model"
      ],
      "metadata": {
        "id": "YvahXkxuQsMj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: DMA\n",
        "decoder = dma_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/dma'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/dma', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/DMAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='DeepMoeAttentionTransformer Mini v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "fGNQ71kxQrEi"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mixture-of-Experts Attention - Next round\n",
        "\n",
        "> Additional epoch to finally confirm that **MoE Attention** is not worth using\n",
        "\n",
        "> Unfortunately Mixture-of-Experts Attention has worse results than classic GQA/MQA, so we abandoned this study and proceed to **Sparse Query Attention** research."
      ],
      "metadata": {
        "id": "4-8FoOqZ2s8M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GQA"
      ],
      "metadata": {
        "id": "xAhoVQRa2wst"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "next_dataset = _rest_dataset"
      ],
      "metadata": {
        "id": "8yVn3IDD3JWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GQA\n",
        "decoder = MoeAttentionTransformer.from_pretrained('ReactiveAI/GQA-MAT-m')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 3e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(next_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/gqa_rest'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/gqa_rest', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GQA-MAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GQA Mini Ref Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(next_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "Nj9GTXoJ2vke"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### GMA"
      ],
      "metadata": {
        "id": "dvQbbjbR4O4H"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GMA\n",
        "decoder = MoeAttentionTransformer.from_pretrained('ReactiveAI/GMAT-m')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 3e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(next_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/gma_rest'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/gma_rest', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GMAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GroupedMoeAttentionTransformer Mini v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(next_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "cI9DEj-d4ZkP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### DMA"
      ],
      "metadata": {
        "id": "DsXbaqJV41zs"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: DMA\n",
        "decoder = MoeAttentionTransformer.from_pretrained('ReactiveAI/DMAT-m')\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 3e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(next_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/dma_rest'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/dma_rest', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/DMAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='DeepMoeAttentionTransformer Mini v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(next_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "n1XYXMKW4356"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SparseQueryAttention - Mini models test\n",
        "\n",
        "> Tests for **SQA** variants with other params same as above Mini models"
      ],
      "metadata": {
        "id": "upEgptqXLzFW"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Sparse Query Attention base"
      ],
      "metadata": {
        "id": "FhAz3isTCStt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "sqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 8\n",
        "}\n",
        "\n",
        "\n",
        "sqa_decoder = ExperimentalAttentionTransformer(**sqa_config)\n",
        "sqa_decoder.params_count()"
      ],
      "metadata": {
        "id": "WHw3_UtHL3k6"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: SQA\n",
        "decoder = sqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/sqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/sqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/SQAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='SQA Mini Ref Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "6FMKWUANMUAn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### SparseQueryAttention - extreme version"
      ],
      "metadata": {
        "id": "UiLe4LXBr-wI"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xsqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 4\n",
        "}\n",
        "\n",
        "\n",
        "xsqa_decoder = ExperimentalAttentionTransformer(**xsqa_config)\n",
        "xsqa_decoder.params_count()"
      ],
      "metadata": {
        "id": "bnTpiweLsFYI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: xSQA\n",
        "decoder = xsqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/xsqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/xsqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/xSQAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='xSQA Mini Transformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "em7QJx-HubE1"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Symmetric SQA Variant"
      ],
      "metadata": {
        "id": "gKYUehLPTaQ1"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ssqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 8,\n",
        "    'att_groups': 8,\n",
        "}\n",
        "\n",
        "\n",
        "ssqa_decoder = ExperimentalAttentionTransformer(**ssqa_config)\n",
        "ssqa_decoder.params_count()"
      ],
      "metadata": {
        "id": "Ph73AEntUNhS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: sSQA\n",
        "decoder = ssqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/ssqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/ssqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/sSQAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='sSQA Mini Transformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "RSSphxFkU-oT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Extreme Sparse Multi Query Attention"
      ],
      "metadata": {
        "id": "My4tD6lrYT1P"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "xsmqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 4,\n",
        "    'att_groups': 1,\n",
        "}\n",
        "\n",
        "\n",
        "xsmqa_decoder = ExperimentalAttentionTransformer(**xsmqa_config)\n",
        "xsmqa_decoder.params_count()"
      ],
      "metadata": {
        "id": "Y_46jTOcY-FX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: xSMQA\n",
        "decoder = xsmqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 128\n",
        "epochs = 1\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 5e-4 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(wiki_train_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(decoder.params_count())\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = int(0.25 * steps_per_epoch)\n",
        "\n",
        "\n",
        "logs_dir = './att_mini/tensorboard_logs/xsmqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./att_mini/xsmqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/xSMQAT-m', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='xSMQA Mini Transformer v1')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(wiki_train_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(wiki_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, gradient_accumulation_steps=gradient_acc_steps, use_moe_aux_loss=False, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "kxLyUzB2ZUva"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Mini models computational performance comparison\n"
      ],
      "metadata": {
        "id": "EEZvQQAGodyY"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "- Nvidia L40S 48GB GPU\n",
        "- 200 steps\n",
        "- 32 batch size\n",
        "```\n",
        "Model:   (time per 200 steps, time per single batch)\n",
        "'GQA':   (3.097075939178467, 0.015485379695892334)\n",
        "'GQAs':  (3.107764959335327, 0.015538824796676636)\n",
        "'SQA':   (2.827148914337158, 0.014135744571685791)\n",
        "'xSQA':  (2.6865649223327637, 0.013432824611663818)\n",
        "'xSMQA': (2.6817448139190674, 0.013408724069595337)\n",
        "'sSQA':  (2.9016659259796143, 0.014508329629898072)\n",
        "'MQA':   (2.898437023162842, 0.014492185115814208)\n",
        "```\n",
        "- 128 batch size\n",
        "```\n",
        " Model:  (time per 200 steps, time per single batch)\n",
        "'GQA':   (19.43833899497986, 0.0971916949748993)\n",
        "'GQAs':  (19.273487091064453, 0.09636743545532227)\n",
        "'SQA':   (16.972065210342407, 0.08486032605171204)\n",
        "'xSQA':  (16.232131004333496, 0.08116065502166749)\n",
        "'xSMQA': (15.909909963607788, 0.07954954981803894)\n",
        "'sSQA':  (17.343858003616333, 0.08671929001808167)\n",
        "'MQA':   (17.349792957305908, 0.08674896478652955)\n",
        "```"
      ],
      "metadata": {
        "id": "9HxzwIBLC26b"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "# device = torch.device('cpu')\n",
        "torch.cuda.empty_cache()\n",
        "print(device)\n",
        "\n",
        "inp = torch.ones(128, 1024, dtype=torch.int32).to(device)\n",
        "gqa_decoder = gqa_decoder.to(device)\n",
        "gqas_decoder = gqas_decoder.to(device)\n",
        "sqa_decoder = sqa_decoder.to(device)\n",
        "xsqa_decoder = xsqa_decoder.to(device)\n",
        "xsmqa_decoder = xsmqa_decoder.to(device)\n",
        "ssqa_decoder = ssqa_decoder.to(device)\n",
        "mqa_decoder = mqa_decoder.to(device)\n",
        "\n",
        "steps = 20\n",
        "warmup = 100\n",
        "\n",
        "results = {\n",
        "    'GQA': (0.0, 0.0),\n",
        "    'GQAs': (0.0, 0.0),\n",
        "    'SQA': (0.0, 0.0),\n",
        "    'xSQA': (0.0, 0.0),\n",
        "    'xSMQA': (0.0, 0.0),\n",
        "    'sSQA': (0.0, 0.0),\n",
        "    'MQA': (0.0, 0.0),\n",
        "}\n",
        "\n",
        "with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "  print(inp.dtype)\n",
        "  for _ in range(warmup):\n",
        "    _ = gqa_decoder(inp)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = gqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['GQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = gqas_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['GQAs'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = sqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['SQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = xsqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['xSQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = xsmqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['xSMQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = ssqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['sSQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  t1 = datetime.timestamp(datetime.now())\n",
        "  for _ in range(steps):\n",
        "    _ = ssqa_decoder(inp)\n",
        "  t2 = datetime.timestamp(datetime.now())\n",
        "  results['MQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "results"
      ],
      "metadata": {
        "id": "eXxHUiIqmSOQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# SQA - Micro MoE models full training test\n",
        "\n",
        "> **SQA** variants tests for micro Mixture-of-Experts architectures, compared to **GQA** and **MQA**"
      ],
      "metadata": {
        "id": "jgrJR9MwZGtP"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init models"
      ],
      "metadata": {
        "id": "JQMGVZNEZSHy"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "embed_dim = 128\n",
        "vocab_size = 5_000\n",
        "seq_len = 256\n",
        "\n",
        "torch.random.manual_seed(2137)\n",
        "np.random.seed(2137)\n",
        "random.seed(2137)\n",
        "\n",
        "base_config = {\n",
        "    'num_layers': 6,\n",
        "    'vocab_size': vocab_size,\n",
        "    'embed_dim': embed_dim,\n",
        "    'att_heads': 8,\n",
        "    'att_groups': 2,\n",
        "    'seq_len': seq_len,\n",
        "    'use_flash_attention': False,\n",
        "    'use_gated': True,\n",
        "    'ff_dropout': 0.1,\n",
        "    'ff_activation': 'silu',\n",
        "    'ff_dim': 256,\n",
        "    'use_moe_ff': True,\n",
        "    'ff_num_experts': 12,\n",
        "    'ff_moe_top_k': 2,\n",
        "}\n",
        "\n",
        "gqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'gqa',\n",
        "}\n",
        "\n",
        "mqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'mqa',\n",
        "}\n",
        "\n",
        "\n",
        "sqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 4\n",
        "}\n",
        "\n",
        "ssqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 4,\n",
        "    'att_groups': 4,\n",
        "}\n",
        "\n",
        "xsqa_config = {\n",
        "    **base_config,\n",
        "    'att_type': 'sqa',\n",
        "    'att_num_query_groups': 2,\n",
        "}\n",
        "\n",
        "\n",
        "gqa_decoder = ExperimentalAttentionTransformer(**gqa_config)\n",
        "mqa_decoder = ExperimentalAttentionTransformer(**mqa_config)\n",
        "sqa_decoder = ExperimentalAttentionTransformer(**sqa_config)\n",
        "ssqa_decoder = ExperimentalAttentionTransformer(**ssqa_config)\n",
        "xsqa_decoder = ExperimentalAttentionTransformer(**xsqa_config)\n",
        "\n",
        "(('GQA', gqa_decoder.params_count()),\n",
        "('MQA', mqa_decoder.params_count()),\n",
        "('SQA', sqa_decoder.params_count()),\n",
        "('sSQA', ssqa_decoder.params_count()),\n",
        "('xSQA', xsqa_decoder.params_count()))"
      ],
      "metadata": {
        "id": "qq4fmFAKZMSg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "tr = TokenizerTrainer.from_pretrained('ReactiveAI/RxT-Alpha-Micro-Decoder')\n",
        "tokenizer = tr.get_hf_tokenizer()\n",
        "tokenizer"
      ],
      "metadata": {
        "id": "ZoXZQaOFbIKV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from datasets import load_dataset\n",
        "stories_dataset = load_dataset('roneneldan/TinyStories', split='train', trust_remote_code=True)\n",
        "hf_valid_dataset = load_dataset('roneneldan/TinyStories', split='validation', trust_remote_code=True)\n",
        "len(stories_dataset), len(hf_valid_dataset)"
      ],
      "metadata": {
        "id": "rUP5plvna4YC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## GQA"
      ],
      "metadata": {
        "id": "hu-5ogIfbQdY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: GQA\n",
        "decoder = gqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 2e-3 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './micro_att/tensorboard_logs/gqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./micro_att/gqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/GQA-Ref-Micro', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='GQA Ref Micro Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "6H_tcUs2bTXD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## MQA"
      ],
      "metadata": {
        "id": "-dsJXzrxbsgh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: MQA\n",
        "decoder = mqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 2e-3 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './micro_att/tensorboard_logs/mqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./micro_att/mqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/MQA-Ref-Micro', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='MQA Ref Micro Transformer v2')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "aBWU7GaJbuDv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SQA"
      ],
      "metadata": {
        "id": "hJDLGFAIbuZG"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: SQA\n",
        "decoder = sqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 2e-3 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './micro_att/tensorboard_logs/sqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./micro_att/sqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/SQAT-mm', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='SQA-mm Transformer v1.0.0')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "UYUda8webvlJ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## sSQA"
      ],
      "metadata": {
        "id": "vZLXNuZUbwLk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: sSQA\n",
        "decoder = ssqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 2e-3 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './micro_att/tensorboard_logs/ssqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./micro_att/ssqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/sSQAT-mm', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='sSQA-mm Transformer v1.0.0')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "VDdBYXxobxXC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## xSQA"
      ],
      "metadata": {
        "id": "xLJAk1DZbxz3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# selected model: xSQA\n",
        "decoder = xsqa_decoder\n",
        "\n",
        "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "print(device)\n",
        "\n",
        "gc.collect()\n",
        "if torch.cuda.is_available():\n",
        "  torch.cuda.empty_cache()\n",
        "\n",
        "batch_size = 256\n",
        "epochs = 5\n",
        "gradient_acc_steps = 1\n",
        "\n",
        "peak_lr = 2e-3 * gradient_acc_steps\n",
        "\n",
        "subset_len = int(len(stories_dataset))\n",
        "\n",
        "steps_per_epoch = int(subset_len / batch_size - 1)\n",
        "\n",
        "print(f'Total steps per epoch: {steps_per_epoch}')\n",
        "\n",
        "total_steps = int((epochs * steps_per_epoch) / gradient_acc_steps)\n",
        "# warmup_steps = int(0.1 * total_steps)\n",
        "warmup_steps = 0\n",
        "\n",
        "\n",
        "logs_dir = './micro_att/tensorboard_logs/xsqa'\n",
        "if not os.path.exists(logs_dir):\n",
        "  os.makedirs(logs_dir)\n",
        "\n",
        "\n",
        "print_cb = PrintLossCallback(batches_per_epoch=steps_per_epoch)\n",
        "count_cb = TokenCounterCallback(8_000_000_000)\n",
        "acc_cb = PrintAccuracyCallback()\n",
        "\n",
        "save_cb = ModelSaveCallback('./micro_att/xsqa', push_to_hub=True,\n",
        "                            hub_model_id='ReactiveAI/xSQAT-mm', private_repo=True,\n",
        "                            push_checkpoint_weights=True, final_commit_message='xSQA-mm Transformer v1.0.0')\n",
        "\n",
        "train_dataset = AutoregressiveLMDataset(stories_dataset, tokenizer, max_seq_len=seq_len)\n",
        "valid_dataset = AutoregressiveLMDataset(hf_valid_dataset, tokenizer, max_seq_len=seq_len)\n",
        "trainer = AutoregressiveTrainer(decoder, device, dataset=train_dataset, validation_dataset=valid_dataset,\n",
        "                         vocab_size=vocab_size, callbacks=[print_cb, acc_cb, count_cb, save_cb], use_amp=True,\n",
        "                         dtype=torch.bfloat16, log_dir=logs_dir, use_moe_aux_loss=True, moe_aux_loss_scale=0.01)\n",
        "optimizer = torch.optim.AdamW(decoder.parameters(), lr=peak_lr, weight_decay=0.01)\n",
        "scheduler = get_transformer_lr_scheduler(\n",
        "    optimizer,\n",
        "    warmup_steps=warmup_steps,\n",
        "    num_training_steps=total_steps\n",
        ")\n",
        "\n",
        "t1 = datetime.timestamp(datetime.now())\n",
        "trainer(epochs=epochs, batch_size=batch_size, optimizer=optimizer, scheduler=scheduler)\n",
        "t2 = datetime.timestamp(datetime.now())\n",
        "\n",
        "training_time = t2 - t1\n",
        "time_per_step = training_time / total_steps\n",
        "\n",
        "f'Total time: {(training_time / 60):.2f}, Time per step/batch: {time_per_step:.4f}'"
      ],
      "metadata": {
        "id": "ZjbayUlIbzTC"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Longer context computation efficiency"
      ],
      "metadata": {
        "id": "QZqx7FGiCk-8"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Init models"
      ],
      "metadata": {
        "id": "QHV7b1VRDYRS"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "torch.random.manual_seed(42)\n",
        "np.random.seed(42)\n",
        "random.seed(42)\n",
        "\n",
        "\n",
        "def create_test_models(seq_len: int = 1024):\n",
        "  embed_dim = 256\n",
        "  vocab_size = 10_000\n",
        "\n",
        "  base_config = {\n",
        "      'num_layers': 8,\n",
        "      'vocab_size': vocab_size,\n",
        "      'embed_dim': embed_dim,\n",
        "      'att_heads': 16,\n",
        "      'att_groups': 4,\n",
        "      'seq_len': seq_len,\n",
        "      'use_flash_attention': False,\n",
        "      'use_gated': True,\n",
        "      'ff_dropout': 0.1,\n",
        "      'ff_activation': 'silu',\n",
        "      'ff_dim': 768,\n",
        "  }\n",
        "\n",
        "  gqa_config = {\n",
        "      **base_config,\n",
        "      'att_type': 'gqa',\n",
        "  }\n",
        "\n",
        "  mqa_config = {\n",
        "      **base_config,\n",
        "      'att_type': 'mqa',\n",
        "  }\n",
        "\n",
        "  mha_config = {\n",
        "      **base_config,\n",
        "      'att_type': 'mha',\n",
        "  }\n",
        "\n",
        "  sqa_config = {\n",
        "      **base_config,\n",
        "      'att_type': 'sqa',\n",
        "      'att_num_query_groups': 8\n",
        "  }\n",
        "\n",
        "  ssqa_config = {\n",
        "      **base_config,\n",
        "      'att_type': 'sqa',\n",
        "      'att_num_query_groups': 8,\n",
        "      'att_groups': 8,\n",
        "  }\n",
        "\n",
        "  xsqa_config = {\n",
        "      **base_config,\n",
        "      'att_type': 'sqa',\n",
        "      'att_num_query_groups': 4\n",
        "  }\n",
        "\n",
        "  gqa_decoder = ExperimentalAttentionTransformer(**gqa_config)\n",
        "  mqa_decoder = ExperimentalAttentionTransformer(**mqa_config)\n",
        "  mha_decoder = ExperimentalAttentionTransformer(**mha_config)\n",
        "  sqa_decoder = ExperimentalAttentionTransformer(**sqa_config)\n",
        "  ssqa_decoder = ExperimentalAttentionTransformer(**ssqa_config)\n",
        "  xsqa_decoder = ExperimentalAttentionTransformer(**xsqa_config)\n",
        "\n",
        "  print((\n",
        "    ('GQA', gqa_decoder.params_count()),\n",
        "    ('MQA', mqa_decoder.params_count()),\n",
        "    ('MHA', mha_decoder.params_count()),\n",
        "    ('SQA', sqa_decoder.params_count()),\n",
        "    ('sSQA', ssqa_decoder.params_count()),\n",
        "    ('xSQA', xsqa_decoder.params_count())\n",
        "  ))\n",
        "\n",
        "  return {\n",
        "      'GQA': gqa_decoder,\n",
        "      'MQA': mqa_decoder,\n",
        "      'MHA': mha_decoder,\n",
        "      'SQA': sqa_decoder,\n",
        "      'sSQA': ssqa_decoder,\n",
        "      'xSQA': xsqa_decoder,\n",
        "  }"
      ],
      "metadata": {
        "id": "TsFDJaVNDbcc"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def time_tests(models: dict[str, ExperimentalAttentionTransformer], batch_size: int = 64, seq_len: int = 1024):\n",
        "  device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
        "  if torch.cuda.is_available():\n",
        "    torch.cuda.empty_cache()\n",
        "\n",
        "  print(device)\n",
        "\n",
        "  inp = torch.randint(1, 9999, (batch_size, seq_len), dtype=torch.int32).to(device)\n",
        "  gqa_decoder = models['GQA'].to(device)\n",
        "  mqa_decoder = models['MQA'].to(device)\n",
        "  mha_decoder = models['MHA'].to(device)\n",
        "  sqa_decoder = models['SQA'].to(device)\n",
        "  ssqa_decoder = models['sSQA'].to(device)\n",
        "  xsqa_decoder = models['xSQA'].to(device)\n",
        "\n",
        "\n",
        "  steps = 50\n",
        "  warmup = 100\n",
        "\n",
        "  results = {\n",
        "      'GQA': (0.0, 0.0),\n",
        "      'MQA': (0.0, 0.0),\n",
        "      'MHA': (0.0, 0.0),\n",
        "      'SQA': (0.0, 0.0),\n",
        "      'sSQA': (0.0, 0.0),\n",
        "      'xSQA': (0.0, 0.0),\n",
        "  }\n",
        "\n",
        "  with torch.amp.autocast(device_type=device.type, dtype=torch.bfloat16):\n",
        "\n",
        "    print(inp.dtype)\n",
        "    for _ in range(warmup):\n",
        "      _ = gqa_decoder(inp)\n",
        "\n",
        "    t1 = datetime.timestamp(datetime.now())\n",
        "    for _ in range(steps):\n",
        "      _ = gqa_decoder(inp)\n",
        "    t2 = datetime.timestamp(datetime.now())\n",
        "    results['GQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "    t1 = datetime.timestamp(datetime.now())\n",
        "    for _ in range(steps):\n",
        "      _ = mqa_decoder(inp)\n",
        "    t2 = datetime.timestamp(datetime.now())\n",
        "    results['MQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "    t1 = datetime.timestamp(datetime.now())\n",
        "    for _ in range(steps):\n",
        "      _ = mha_decoder(inp)\n",
        "    t2 = datetime.timestamp(datetime.now())\n",
        "    results['MHA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "    t1 = datetime.timestamp(datetime.now())\n",
        "    for _ in range(steps):\n",
        "      _ = sqa_decoder(inp)\n",
        "    t2 = datetime.timestamp(datetime.now())\n",
        "    results['SQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "    t1 = datetime.timestamp(datetime.now())\n",
        "    for _ in range(steps):\n",
        "      _ = ssqa_decoder(inp)\n",
        "    t2 = datetime.timestamp(datetime.now())\n",
        "    results['sSQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "    t1 = datetime.timestamp(datetime.now())\n",
        "    for _ in range(steps):\n",
        "      _ = xsqa_decoder(inp)\n",
        "    t2 = datetime.timestamp(datetime.now())\n",
        "    results['xSQA'] = (t2 - t1, (t2 - t1) / steps)\n",
        "\n",
        "  return results"
      ],
      "metadata": {
        "id": "5Is_oqXJG-aY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 1024 Sequence / 128 batch size / 50 steps"
      ],
      "metadata": {
        "id": "7G03YJhBJQ5x"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_tests(create_test_models(1024), 128, 1024)"
      ],
      "metadata": {
        "id": "OJtzYSQTHY3D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 4096 Sequence / 32 batch size / 50 steps"
      ],
      "metadata": {
        "id": "4LlEU5sKKoek"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_tests(create_test_models(4096), 32, 4096)"
      ],
      "metadata": {
        "id": "oco_AUjIKtvG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 32768 Sequence / 4 batch size / 50 steps"
      ],
      "metadata": {
        "id": "I-KpDJuYLaqT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_tests(create_test_models(32768), 4, 32768)"
      ],
      "metadata": {
        "id": "Yq6usUXMLkk8"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### 131072 Sequence / 1 batch size / 50 steps"
      ],
      "metadata": {
        "id": "u6P06Z_KMoI7"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "time_tests(create_test_models(131072), 1, 131072)"
      ],
      "metadata": {
        "id": "BdwL_RMfMszh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}